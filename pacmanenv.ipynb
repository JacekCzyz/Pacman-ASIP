{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import gym \n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "from run import GameController\n",
    "from constants import *\n",
    "from pacman import Pacman\n",
    "from ghost import Ghosts\n",
    "from nodes import NodeGroup\n",
    "from pellets import PelletGroup\n",
    "from fruits import Fruits\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "import time\n",
    "class PacmanEnv(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "class PacmanEnv(gym.Env):\n",
    "    def __init__(self, render_mode=False):\n",
    "        super(PacmanEnv, self).__init__()\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        if not render_mode:\n",
    "            os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "        else:\n",
    "            os.environ.pop(\"SDL_VIDEODRIVER\", None)\n",
    "\n",
    "        pygame.quit() \n",
    "        pygame.init()\n",
    "\n",
    "        self.game = GameController(render_mode=render_mode)\n",
    "\n",
    "        self.action_space = spaces.Discrete(5, start=-2)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=255, shape=(SCREENHEIGHT, SCREENWIDTH, 3), dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.game.startGame(3)\n",
    "        state = self.get_observation()\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        action = int(action)\n",
    "        action = np.clip(action, 0, 4)\n",
    "    \n",
    "        action = action - 2\n",
    "        if self.game.pacman.validDirection(action):\n",
    "            self.game.pacman.direction = action \n",
    "\n",
    "        pelletBefore = self.game.pellets.numEaten \n",
    "        lifesBefore = self.game.pacman.life_amount   \n",
    "        self.game.update()\n",
    "\n",
    "        if self.game.pacman.target is not None and self.game.pacman.overshotTarget():\n",
    "            self.game.pacman.node = self.game.pacman.target\n",
    "            self.game.pacman.setPosition()\n",
    "\n",
    "        self.game.update() \n",
    "\n",
    "        state = self.get_observation() \n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        pellet = self.game.pellets.numEaten - pelletBefore\n",
    "        if pellet == 1:\n",
    "            reward += 20\n",
    "\n",
    "        fruit = None\n",
    "        if self.game.fruits is not None:\n",
    "            fruit = self.game.pacman.eatFruits(self.game.fruits)\n",
    "            if fruit:\n",
    "                reward += 20\n",
    "\n",
    "        lifes = self.game.pacman.life_amount - lifesBefore\n",
    "        if lifes == -1:\n",
    "            reward -= 50\n",
    "\n",
    "\n",
    "        if pellet == 0 and fruit is None:\n",
    "            reward -= 2\n",
    "\n",
    "        done = self.check_game_over()\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return state, reward, done, info\n",
    "    \n",
    "    def render(self, mode=\"human\"):\n",
    "        if self.render_mode and mode == \"human\":\n",
    "            self.game.render()\n",
    "\n",
    "    def get_observation(self):\n",
    "        return pygame.surfarray.array3d(self.game.screen)\n",
    "\n",
    "    def _init_pygame(self):\n",
    "        if not pygame.get_init():\n",
    "            pygame.init()\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()\n",
    "\n",
    "    def check_game_over(self):\n",
    "        return self.game.pacman.life_amount == 0\n",
    "    \n",
    "    def change_resolution(self, width, height):\n",
    "        global SCREENWIDTH, SCREENHEIGHT\n",
    "\n",
    "        constants_path = os.path.join(os.path.dirname(__file__), \"constants.py\")\n",
    "        with open(constants_path, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        with open(constants_path, \"w\") as file:\n",
    "            for line in lines:\n",
    "                if line.startswith(\"SCREENWIDTH\"):\n",
    "                    file.write(f\"SCREENWIDTH = {width}\\n\")\n",
    "                elif line.startswith(\"SCREENHEIGHT\"):\n",
    "                    file.write(f\"SCREENHEIGHT = {height}\\n\")\n",
    "                else:\n",
    "                    file.write(line)\n",
    "\n",
    "        SCREENWIDTH, SCREENHEIGHT = width, height\n",
    "        \n",
    "        self.game.screen = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT))\n",
    "        self.game.width, self.game.height = SCREENWIDTH, SCREENHEIGHT\n",
    "\n",
    "    def get_observation(self):\n",
    "        observation = pygame.surfarray.array3d(self.game.screen)\n",
    "        return np.transpose(observation, (1, 0, 2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Ghosts.update_ghosts() missing 1 required positional argument: 'pacman'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining the model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     env\u001b[38;5;241m.\u001b[39mreset\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n",
      "File \u001b[1;32mc:\\Users\\jacek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[0;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jacek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jacek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:560\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jacek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jacek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_transpose.py:97\u001b[0m, in \u001b[0;36mVecTransposeImage.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m---> 97\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# Transpose the terminal observations\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, done \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dones):\n",
      "File \u001b[1;32mc:\\Users\\jacek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\jacek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32mc:\\Users\\jacek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shimmy\\openai_gym_compatibility.py:250\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "Cell \u001b[1;32mIn[2], line 55\u001b[0m, in \u001b[0;36mPacmanEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     53\u001b[0m pelletBefore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mpellets\u001b[38;5;241m.\u001b[39mnumEaten \n\u001b[0;32m     54\u001b[0m lifesBefore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mpacman\u001b[38;5;241m.\u001b[39mlife_amount   \n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mpacman\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mpacman\u001b[38;5;241m.\u001b[39movershotTarget():\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mpacman\u001b[38;5;241m.\u001b[39mnode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mpacman\u001b[38;5;241m.\u001b[39mtarget\n",
      "File \u001b[1;32mc:\\Users\\jacek\\source\\repos\\Pacman\\run.py:66\u001b[0m, in \u001b[0;36mGameController.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;241m30\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000.0\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpacman\u001b[38;5;241m.\u001b[39mupdate(dt)\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mghosts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_ghosts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpellets\u001b[38;5;241m.\u001b[39mupdate(dt)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfruits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Ghosts.update_ghosts() missing 1 required positional argument: 'pacman'"
     ]
    }
   ],
   "source": [
    "env = PacmanEnv(render_mode=False)\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1, buffer_size=1000)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "for i in range(10):\n",
    "    model.learn(total_timesteps=2000)\n",
    "    env.reset\n",
    "    print(i)\n",
    "\n",
    "model.save(\"pacman_10x2000dqn_model\")\n",
    "env.close()\n",
    "\n",
    "model = DQN(\"MlpPolicy\", env, verbose=0, buffer_size=1000)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "for i in range(20):\n",
    "    model.learn(total_timesteps=1000)\n",
    "    env.reset\n",
    "    print(i)\n",
    "\n",
    "model.save(\"pacman_20x1000dqn_model\")\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching to testing mode...\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "starting episode0\n",
      "Episode 0 exceeded 10 minutes, stopping early.\n",
      "starting episode1\n",
      "Episode 1 exceeded 10 minutes, stopping early.\n",
      "starting episode2\n",
      "Episode 2 exceeded 10 minutes, stopping early.\n",
      "starting episode3\n",
      "starting episode4\n",
      "starting episode5\n",
      "starting episode6\n",
      "starting episode7\n",
      "starting episode8\n",
      "Episode 8 exceeded 10 minutes, stopping early.\n",
      "starting episode9\n",
      "Episode 9 exceeded 10 minutes, stopping early.\n",
      "starting episode10\n",
      "starting episode11\n",
      "Episode 11 exceeded 10 minutes, stopping early.\n",
      "starting episode12\n",
      "starting episode13\n",
      "Episode 13 exceeded 10 minutes, stopping early.\n",
      "starting episode14\n",
      "Episode 14 exceeded 10 minutes, stopping early.\n",
      "starting episode15\n",
      "Episode 15 exceeded 10 minutes, stopping early.\n",
      "starting episode16\n",
      "starting episode17\n",
      "starting episode18\n",
      "Episode 18 exceeded 10 minutes, stopping early.\n",
      "starting episode19\n"
     ]
    }
   ],
   "source": [
    "print(\"Switching to testing mode...\")\n",
    "env = PacmanEnv(render_mode=False)\n",
    "state = env.reset()\n",
    "rewardMain = 0\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1, buffer_size=1000)\n",
    "\n",
    "model.load(\"pacman_10000_dqn_model.zip\")\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "rewards = []\n",
    "durations = []\n",
    "times=[]\n",
    "episodes = 20\n",
    "max_episode_time = 10 * 60\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    steps_time = 0\n",
    "    done = False\n",
    "    episode_start_time = time.time()\n",
    "    print(\"starting episode\" + str(episode))\n",
    "    while not done:\n",
    "        if time.time() - episode_start_time >= max_episode_time:\n",
    "            print(f\"Episode {episode} exceeded 10 minutes, stopping early.\")\n",
    "            break\n",
    "        steptime = time.time()\n",
    "        action, _ = model.predict(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        steps_time += time.time() - steptime\n",
    "        steps += 1\n",
    "        env.render()\n",
    "    times.append(steps_time/steps)\n",
    "    rewards.append(episode_reward)\n",
    "    durations.append(steps)\n",
    "env.close()\n",
    "\n",
    "file = open(\"tests.txt\", \"a\")\n",
    "file.write(\"DQN model for \" + str(episodes) + \" episodes\\n\")    \n",
    "for i in range(len(rewards)):\n",
    "    file.write(\"episode\" + str(i)+ \" :\" + \"reward \" + str(rewards[i]) + \" duration: \" + str(durations[i]) + \" mean step time: \" + str(times[i])+\"\\n\")     \n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PacmanEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_vec_env\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the Pacman environment in training mode\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mPacmanEnv\u001b[49m(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Use training mode (no rendering)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize PPO model with CnnPolicy\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, ent_coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PacmanEnv' is not defined"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "env = PacmanEnv(render_mode=False)\n",
    "    \n",
    "model = PPO(\"CnnPolicy\", env, verbose=1, n_steps=256, batch_size=64, ent_coef=0.01)\n",
    "\n",
    "print(\"Training the PPO model...\")\n",
    "model.learn(total_timesteps=100)\n",
    "model.save(\"pacman_ppo_model\")\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Switching to testing mode...\")\n",
    "env = PacmanEnv(render_mode=True)\n",
    "state = env.reset()\n",
    "rewardMain = 0\n",
    "model = PPO(\"CnnPolicy\", env, verbose=1, n_steps=256, batch_size=64, ent_coef=0.01)\n",
    "\n",
    "model.load(\"pacman_ppo_model.zip\")\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "rewards = []\n",
    "durations = []\n",
    "episodes = 15\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    rewards.append(episode_reward)\n",
    "    durations.append(steps)\n",
    "\n",
    "file = open(\"tests.txt\")\n",
    "file.write(\"ppo model for {episodes} episodes\")    \n",
    "for i in range(rewards.count()):\n",
    "    file.write(\"episode {i}:\", \"reward \",rewards[i], \" duration: \", durations[i])    \n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trained PPO model:\")\n",
    "state = env.reset()\n",
    "rewardMain = 0\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(state)\n",
    "\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "    rewardMain += reward\n",
    "\n",
    "    if done:\n",
    "        print(\"Game Over\")\n",
    "        break\n",
    "\n",
    "print(f\"Total reward during testing: {rewardMain}\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
